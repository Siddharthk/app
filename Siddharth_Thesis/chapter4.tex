

\section{Independent Component Analysis (ICA) Model}
\label{sec:format}
 

\begin{figure}[t]
\centering
\subfigure{
\includegraphics[height=3.8in,width=5.9in]{chap4/res_7/framework.eps}
}
\caption
{ICA Model - IC1: Independent component containing the foreground text, IC2: Independent
component containing the Background, IC3: Independent component containing the mixture of foreground and background.
y $\in$ $\{$R,G,B$\}$}
\label{fig:frame}
\end{figure}
Independent Component Analysis (ICA) has been an active research topic 
because of its potential applications in signal processing. And recently 
it has received attention in image processing tasks.
In ICA model, more than one observation signals are needed to achieve the 
analysis.
Currently when ICA is applied in image processing, images are divided into blocks \cite{chap4-1,chap4-2,chap4-3}  
with size of 8x8 or 16x16. These blocks are taken as the observations of ICA model.
But here we divide the image into sub-components i.e Red, Green and blue channels which are
taken as the observation of ICA model (Fig \ref{fig:frame}). 

The goal of ICA is to separate independent 
source signals from the observed signals, which is assumed 
to be the linear mixtures of independent source components. 
The mathematical model of ICA is formulated by 
mixture processing and an explicit decomposition processing.  
Assume there exists a set of `n' unknown source signals 
$S=\{s_1, s_2,..., s_n \}$. The assumptions of the components 
$\{s_i\}$  include mutual independence, stationary and zero mean. 
A set of observed signals $X=\{x_1,x_2,...,x_n\}$, are regarded as the mixture of the source 
components. The most frequently considered 
mixing model is the linear instantaneous noise free model,
which is described as: 
\begin{equation}
x_i=\sum_{j=1}^{n}a_{ij}s_j 
\end{equation}or in the matrix notation
\begin{equation}
X=A.S  
\end{equation}where A is an unknown full rank mixing matrix, which is 
also called mixture matrix. Eqn 4.1 assumes that there exists a 
linear relationship between the sources $S$ and the observations 
$X$. The ICA model describes how the observed data is 
generated by a process of mixing the components. In our case, `n' is equal to 3. 


\section{Natural Scene Text Binarization}
Binarization is a process to convert a grayscale or color 
images into black and white images. Binarization 
problem classifies individual pixels as foreground text or 
background. A pixel is grouped into two colors i.e black and white. The black pixels represent the 
foreground text while the white pixels represent the background and thus a binary image is created.
It is a necessary step before recognition of text by OCR (Optical Character Recognition). 
Accurate segmentation of natural scene texts can significantly increase the success of
the subsequent text recognition step. 
But these scene texts contain numerous degradations 
such as noise, uneven
illumination, blur, highlights, shadows,  multiple colors and complex textured background.
These issues severely degrade the segmentation accuracy. Hence we propose a technique which helps is 
segmentation of text from complex backgrounds.
Fig \ref{fig:sample} shows the sample images that we considered in this work.

\begin{figure}[t]
\centering
\subfigure[]{
\includegraphics[height=.6in,width=1.8in]{results/res_3/1.eps}
\includegraphics[height=.6in,width=1.8in]{results/res_3/2.eps}
\includegraphics[height=.6in,width=1.8in]{results/res_3/6.eps}
\label{fig:subfig11}
}
\subfigure[]{
\includegraphics[height=.6in,width=1.8in]{results/res_3/3.eps}
\includegraphics[height=.6in,width=1.8in]{results/res_3/4.eps}
\includegraphics[height=.6in,width=1.8in]{results/res_3/8.eps}
\label{fig:subfig12}
}
\subfigure[]{
\includegraphics[height=.6in,width=1.8in]{results/res_3/5.eps}
\includegraphics[height=.6in,width=1.8in]{results/res_3/7.eps}
\includegraphics[height=.6in,width=1.8in]{results/res_3/10.eps}
\label{fig:subfig13}
}
\caption
{Some sample word images we considered in this work containing (a) reflective (b) shadowed and (c) specular background}
\label{fig:sample}
\end{figure}
\begin{figure}[t]
\centering
\subfigure{
\includegraphics[height=1.9in,width=4in]{results/framework.eps}

\label{fig:subfig11}
}
\caption
{Framework for the proposed method}
\label{fig:3}
\end{figure}

\begin{figure*}[t]
\centering
\subfigure[]{
\includegraphics[height=.6in,width=1.8in]{results/res_5/1.eps}
\includegraphics[height=.6in,width=1.8in]{results/res_5/2.eps}
\includegraphics[height=.6in,width=1.8in]{results/res_5/3.eps}
\label{fig:subfig11}
}
\subfigure[]{
\includegraphics[height=.6in,width=1.8in]{results/res_5/4.eps}
\includegraphics[height=.6in,width=1.8in]{results/res_5/5.eps}
\includegraphics[height=.6in,width=1.8in]{results/res_5/6.eps}
\label{fig:subfig12}
}
\subfigure[]{
\includegraphics[height=.6in,width=1.8in]{results/res_5/7.eps}
\includegraphics[height=.6in,width=1.8in]{results/res_5/8.eps}
\includegraphics[height=.6in,width=1.8in]{results/res_5/9.eps}
\label{fig:subfig12}
}
\caption
{Foreground and Background Extracted: (a) Shadowed background and foreground text
(b) Reflective background and foreground text (c) Specular background and foreground text }
\label{fig:extract}
\end{figure*}

\subsection{Binarization process}
\begin{figure*}[t]
\centering
\subfigure[Input]{
\includegraphics[height=.6in, width=1.4in]{results/IC/orig.eps}
\label{fig:subfig11}
}
\subfigure[Red]{
\includegraphics[height=.6in, width=1.4in]{results/IC/R.eps}
\label{fig:subfig12}
}
\subfigure[$IC_1$]{
\includegraphics[height=.6in, width=1.4in]{results/IC/14_1.eps}
\label{fig:subfig11}
}
\subfigure[Binarized $IC_1$]{
\includegraphics[height=.6in, width=1.4in]{results/IC/14_1_binary.eps}
\label{fig:subfig13}
}\\
\hspace*{1.5in}
\subfigure[Green]{
\includegraphics[height=.6in, width=1.4in]{results/IC/G.eps}
\label{fig:subfig13}
}
\subfigure[$IC_2$]{
\includegraphics[height=.6in, width=1.4in]{results/IC/14_2.eps}
\label{fig:subfig12}
}
\subfigure[Binarized $IC_2$]{
\includegraphics[height=.6in, width=1.4in]{results/IC/14_2_binary.eps}
\label{fig:subfig13}
}\\
\hspace*{1.5in}
\subfigure[Blue]{
\includegraphics[height=.6in, width=1.4in]{results/IC/B.eps}
\label{fig:subfig14} } 
\subfigure[$IC_3$]{
\includegraphics[height=.6in, width=1.4in]{results/IC/14.eps}
\label{fig:subfig13}
}
\subfigure[Binarized $IC_3$]{
\includegraphics[height=.6in, width=1.4in]{results/IC/binary.eps}
\label{fig:subfig14} } 
\caption
{(a) Original word image (b),(e),(h) R, G and B channel respectively
(c),(f),(i) Independent Components, (d),(g),(j) Binarized image}
\label{fig:4}
\end{figure*}

A wide variety of ICA algorithms are available in the literature \cite{A11,A12}. These algorithms differ from each other on the
basis of the choice of objective function and selected optimization scheme. Here we use a fast fixed point
ICA algorithm to separate out the text from complex background in images. A Blind Source 
Separation method based on SVD \cite{A10} can also be used. Fig. \ref{fig:3} 
shows the complete framework for the proposed method.

\subsubsection{The Separation Model}

Consider the text image as a mixture of pixels from three different sources
and assume it to be a noiseless instantaneous mixture.
We use a single image i.e its R, G and B channels as three observed signals.
Therefore, we can define that the color intensity at each pixel from these three observed signals
mix linearly to give the resultant color intensity at that pixel.
Denoting these mixture images in row vector form
as $x_r$, $x_g$ and $x_b$, the linear mixing of the sources at a particular pixel $k$ can be
expressed in matrix form as follows:

\begin{equation}
%\resizebox{.91\hsize}{!}{$
\underbrace{\left[ {\begin{array}{c}
{x_r(k)} \\
{x_g(k)} \\
{x_b(k)}
 \end{array} } \right]}_{X}
=
\underbrace{\left[ {\begin{array}{ccc}
a_{11} & a_{12} & a_{13} \\
a_{21} & a_{22} & a_{23} \\
a_{31} & a_{32} & a_{33} \\
 \end{array} } \right]}_{A}
\underbrace{\left[ {\begin{array}{c}
s_1(k)\\s_2(k)\\ s_3(k)
 \end{array} } \right]}_{S}
%$}
\end{equation} where $X$ is an instantaneous linear mixture of source
images at pixel $k$, A is the instantaneous $3$x$3$ square mixing matrix and S
is the source images which add up to form the color intensity 
at pixel $k$.  
The mixed images
in $X$ contain a linear combination of the source
images in $S$. We find the mixing matrix A and sources S using fixed point ICA
algorithm.
Derivation of the algorithm is beyond the scope of this paper.
The reader is encouraged to refer \cite{A11} for this. We summarize the 
fixed point ICA method in Algorithm 2.

\begin{algorithm}[t]
\caption{Fixed Point ICA}
\begin{algorithmic}[1] 
\REQUIRE $X$
\STATE Random initialization of $A$
\STATE $S=A^TX$
\STATE $A^+=Xg(S)^T$
where $g(x)=tanh(x)$ 
\STATE $A=A^+/||A^+||$  
\STATE If not converged, go back to 2.
\ENSURE $A,S$ 
\end{algorithmic}
\end{algorithm} 
From this step, we get three independent sources or components.
Fig. \ref{fig:extract} shows the background and the foreground extracted.
The resultant independent components for a particular word image can be seen in 
Fig. \ref{fig:4} which shows the independent component free from reflective background 
and containing maximum information of the foreground text.

\subsubsection{Thresholding}

Otsu thresholding \cite{A2} is a well-known algorithm that
determines a global threshold for an image by minimizing 
the within-class variance for the resulting classes (foreground pixels 
and background pixels). This is
done by equivalently maximizing the between-class variance
$\sigma _{B}^{2}(T)$ for a given threshold T:
\begin{equation}
\sigma_{B}^{2}=\alpha_1(T)\alpha_2(T)[\mu_1(T)-\mu_2(T)]^2 
\end{equation}
where $\alpha_i$ denotes the number of pixels in each class, $\mu_i$ denotes 
the mean of each class, and T is the value of the potential threshold. 
We apply this thresholding algorithm on all the three independent components to get the binarized image (Fig. \ref{fig:4}).
We can also apply Kittler \cite{A5} algorithm which is also a global thresholding method.
\begin{figure}[tp]
\centering
\subfigure[]{
\includegraphics[height=.6in,width=2in]{results/res_4/orig.eps}
}
\subfigure[]{
\includegraphics[height=.6in,width=2in]{results/res_4/ic.eps}
}
\subfigure[]{
\includegraphics[height=.6in,width=2in]{results/res_4/otsu.eps}
}
\subfigure[]{
\includegraphics[height=.6in,width=2in]{results/res_4/nib.eps}
\label{fig:subfig12}
}
\caption
{(a) Text containing specular highlight (b) IC (c) Otsu (d) Niblack}
\label{fig:5}
\end{figure}

\begin{figure*}[t]
\centering
\subfigure{
\includegraphics[height=.5in,width=1.1in]{chap4/threshold/res5/orig.eps}
}
\subfigure{
\includegraphics[height=.5in,width=1.1in]{chap4/threshold/res5/kit.eps}
}
\subfigure{
\includegraphics[height=.5in,width=1.1in]{chap4/threshold/res5/nib.eps}
}
\subfigure{
\includegraphics[height=.5in,width=1.1in]{chap4/threshold/res5/otsu.eps}
}
\subfigure{
\includegraphics[height=.5in,width=1.1in]{chap4/threshold/res5/sau.eps}
}
\subfigure{
\includegraphics[height=.5in,width=1.1in]{chap4/threshold/res6/orig.eps}
}
\subfigure{
\includegraphics[height=.5in,width=1.1in]{chap4/threshold/res6/kit.eps}
}
\subfigure{
\includegraphics[height=.5in,width=1.1in]{chap4/threshold/res6/nib.eps}
}
\subfigure{
\includegraphics[height=.5in,width=1.1in]{chap4/threshold/res6/otsu.eps}
}
\subfigure{
\includegraphics[height=.5in,width=1.1in]{chap4/threshold/res6/sau.eps}
}
\subfigure{
\includegraphics[height=.5in,width=1.1in]{chap4/threshold/res7/orig.eps}
}
\subfigure{
\includegraphics[height=.5in,width=1.1in]{chap4/threshold/res7/kit.eps}
}
\subfigure{
\includegraphics[height=.5in,width=1.1in]{chap4/threshold/res7/nib.eps}
}
\subfigure{
\includegraphics[height=.5in,width=1.1in]{chap4/threshold/res7/otsu.eps}
}
\subfigure{
\includegraphics[height=.5in,width=1.1in]{chap4/threshold/res7/sau.eps}
}
\subfigure{
\includegraphics[height=.5in,width=1.1in]{chap4/threshold/res8/orig.eps}
}
\subfigure{
\includegraphics[height=.5in,width=1.1in]{chap4/threshold/res8/kit.eps}
}
\subfigure{
\includegraphics[height=.5in,width=1.1in]{chap4/threshold/res8/nib.eps}
}
\subfigure{
\includegraphics[height=.5in,width=1.1in]{chap4/threshold/res8/otsu.eps}
}
\subfigure{
\includegraphics[height=.5in,width=1.1in]{chap4/threshold/res8/sau.eps}
}
\caption
{Failure cases for thresholding based methods. 
From left to right (a) Text Image (b) kittler (c) Niblack (d) Otsu (e) Sauvola}
\label{fig:bad}
%\end{figure*}
%\begin{figure*}[p]
\vspace{10mm}
\centering
\subfigure[]{
\includegraphics[height=.5in,width=1in]{chap4/ground/7.eps}
\includegraphics[height=.5in,width=1in]{chap4/ground/8.eps}
\includegraphics[height=.5in,width=1in]{chap4/ground/9.eps}
\includegraphics[height=.5in,width=1in]{chap4/ground/10.eps}
\includegraphics[height=.5in,width=1in]{chap4/ground/11.eps}
}
\subfigure[]{
\includegraphics[height=.5in,width=1in]{chap4/ground/1.eps}
\includegraphics[height=.5in,width=1in]{chap4/ground/2.eps}
\includegraphics[height=.5in,width=1in]{chap4/ground/3.eps}
\includegraphics[height=.5in,width=1in]{chap4/ground/4.eps}
\includegraphics[height=.5in,width=1in]{chap4/ground/5.eps}
}
%\label{fig:subfig11}
\caption
{(a) Scene Text Image (b) Ground Truth Binary Image}
\label{fig:ground}
\end{figure*}

To find the IC that contains the foreground text, we examine the connected components (CC) in the 
binarization of each IC. For each binarized image, we extract the following features from the CCs: 
average aspect ratio, variance of  CC size, and the deviation from linearity of their centroids.
A simple linear classifier is designed to separate the text and non-text classes in the above feature space.
After binarization, we identify the
connected components and remove non-text portions based on size and aspect ratio.

In some cases where the text image is severely degraded and contain different colored text,
adaptive thresholding methods work better and produce good results. As shown in Fig. \ref{fig:5}, adaptive thresholding
method may perform better than global one. However, in practice we note that a simpler global thresholding
scheme works well in most cases.

\subsection{Experimental Results and Analysis}

We used the ICDAR 2003 Robust Word Recognition Dataset \cite{A15} for our experiments.
The dataset contains a set of JPEG images of single words (Sample (171 words), 
TrialTrain (1157 words) and TrialTest (1111 words)). For qualitative evaluation,
we selected the word images that had complex reflective, shadowed and specular background.  
We separate these word images into Red, Green and Blue channels
assuming that these are the mixture images of the independent source images
that contains the foreground (text) and background. These three images are used
for extracting the foreground as described before.

\begin{table}[ht]
\centering
 \caption{Quantitative Results (Average)}
 \scalebox{1.3}{
  \begin{tabular}{| l | c | c | c |}
    \hline
    Method & Precision & Recall & F-score \\ \hline
    Otsu \cite{A2} & .68 & .75 & 69.17 \\ \hline
    Sauvola \cite{A6} & .63 & .81 & 66.94 \\ \hline
    Kittler \cite{A5} & .66 & .76 & 64.33 \\ \hline    
    Niblack \cite{A9} & .70 & .76 & 71.32 \\ \hline   
    MRF \cite{A16} & .79 & .86 & 80.38 \\ \hline
    Proposed &  .86 & .83 & 83.60\\ \hline
  \end{tabular}}
\label{fig:tab1}
\end{table}
\begin{table}[ht]
\centering
 \caption{OCR Accuracy (\%)}
 \scalebox{1.3}{
  \begin{tabular}{| l | c | c | c |}
    \hline
    Method & Word Accuracy\\ \hline
    MRF \cite{A16} & 43.2 \\ \hline
    Proposed &  61.6\\ \hline
  \end{tabular}}
\label{fig:tab2}
\end{table}

We compare the performance of our method with four well known thresholding algorithms 
i.e Kittler \cite{A5}, Otsu \cite{A2}, Niblack \cite{A9} and Sauvola \cite{A6}.
In the presence of complex
background, the segmentation accuracy of the thresholding based methods decreases (Fig \ref{fig:bad}).
We also compare with the recent method by Mishra \emph{et al} \cite{A16}.
It although performs well for many images but severely
fails in cases of shadows, high illumination variations 
in the image. This poor show is likely due to fact that
performance of the algorithm heavily depends on initial seeds.
We show both qualitative and quantitative results of the proposed method. 
\begin{figure}[p]
\centering
\subfigure{
\includegraphics[height=3.5in,width=5in]{chap4/res_7/pixel_grid.eps}
}
\label{fig:accuracy}
\caption
{Pixel Grid showing precision and recall in a word image}
\vspace{7mm}
\subfigure{
\includegraphics[height=3.5in,width=5in]{chap4/res_7/ocr_recog.eps}
}
\label{fig:ocr}
\caption
{OCR results on scene text images}
\end{figure}
\begin{figure*}[p]
\centering

\subfigure{
\includegraphics[height=.6in,width=1.8in]{results/res_2/1/orig.eps}
\includegraphics[height=.6in,width=1.8in]{results/res_2/1/mrf.eps}
%\subfigure{
%\includegraphics[height=.55in,width=1.2in]{results/res_2/1/kit.eps}
%}
%\subfigure{
%\includegraphics[height=.55in,width=1.2in]{results/res_2/1/otsu.eps}
%}
%\subfigure{
%\includegraphics[height=.55in,width=1.2in]{results/res_2/1/nib.eps}
%}
%\includegraphics[height=.7in,width=1.8in]{results/res_2/1/graph.eps}
\includegraphics[height=.6in,width=1.8in]{results/res_2/1/res.eps}
}
%\label{fig:subfig11}
\subfigure{
\includegraphics[height=.6in,width=1.8in]{results/res_2/2/orig.eps}
\includegraphics[height=.6in,width=1.8in]{results/res_2/2/mrf.eps}
%\subfigure{
%\includegraphics[height=.55in,width=1.2in]{results/res_2/2/kit.eps}
%}
%\subfigure{
%\includegraphics[height=.55in,width=1.2in]{results/res_2/2/otsu.eps}
%}
%\subfigure{
%\includegraphics[height=.55in,width=1.2in]{results/res_2/2/nib.eps}
%}
%\includegraphics[height=.55in,width=1.3in]{results/res_2/2/graph.eps}
\includegraphics[height=.6in,width=1.8in]{results/res_2/2/res.eps}
}
%\\
%\label{fig:subfig11}
\subfigure{
\includegraphics[height=.6in,width=1.8in]{results/res_2/3/orig.eps}
\includegraphics[height=.6in,width=1.8in]{results/res_2/3/mrf.eps}
%\subfigure{
%\includegraphics[height=.5in,width=1in]{results/res_2/3/kit.eps}
%}
%\subfigure{
%\includegraphics[height=.5in,width=1in]{results/res_2/3/otsu.eps}
%}
%\subfigure{
%\includegraphics[height=.5in,width=1in]{results/res_2/3/nib.eps}
%}
%\includegraphics[height=.55in,width=1.3in]{results/res_2/3/graph.eps}
\includegraphics[height=.6in,width=1.8in]{results/res_2/3/res.eps}
}
%\\
%\label{fig:subfig11}
\subfigure{
\includegraphics[height=.6in,width=1.8in]{results/res_2/12/orig.eps}
\includegraphics[height=.6in,width=1.8in]{results/res_2/12/mrf.eps}
%\subfigure{
%\includegraphics[height=.5in,width=1in]{results/res_2/12/kit.eps}
%}
%\subfigure{
%\includegraphics[height=.5in,width=1in]{results/res_2/12/otsu.eps}
%}
%\subfigure{
%\includegraphics[height=.5in,width=1in]{results/res_2/12/nib.eps}
%}
%\includegraphics[height=.55in,width=1.3in]{results/res_2/12/graph.eps}
\includegraphics[height=.6in,width=1.8in]{results/res_2/12/res.eps}
}
%\\
%\label{fig:subfig11}
%
%\caption
%{Comparison of Binarization algorithms and the proposed method 
%(From left to right Original, MRF, Kittler, Otsu, Niblack, Sauvola, Proposed)
%Text containing reflective background
%}
%\label{fig:6}
%\end{figure*}
%\begin{figure*}[t]
%\centering
%
\subfigure{
\includegraphics[height=.6in,width=1.8in]{results/res_2/5/orig.eps}
\includegraphics[height=.6in,width=1.8in]{results/res_2/5/mrf.eps}
%\subfigure{
%\includegraphics[height=.5in,width=1in]{results/res_2/5/kit.eps}
%}
%\subfigure{
%\includegraphics[height=.5in,width=1in]{results/res_2/5/otsu.eps}
%}
%\subfigure{
%\includegraphics[height=.5in,width=1in]{results/res_2/5/nib.eps}
%}
%\includegraphics[height=.55in,width=1.3in]{results/res_2/5/graph.eps}
\includegraphics[height=.6in,width=1.8in]{results/res_2/5/res.eps}
}
%\\
%\label{fig:subfig11}
\subfigure{
\includegraphics[height=.6in,width=1.8in]{results/res_2/6/orig.eps}
\includegraphics[height=.6in,width=1.8in]{results/res_2/6/mrf.eps}
%\subfigure{
%\includegraphics[height=.5in,width=1in]{results/res_2/6/kit.eps}
%}
%\subfigure{
%\includegraphics[height=.5in,width=1in]{results/res_2/6/otsu.eps}
%}
%\subfigure{
%\includegraphics[height=.5in,width=1in]{results/res_2/6/nib.eps}
%}
%\includegraphics[height=.55in,width=1.3in]{results/res_2/6/graph.eps}
\includegraphics[height=.6in,width=1.8in]{results/res_2/6/res.eps}
}
%\\
%\label{fig:subfig11}
%\caption
%{Comparison of Binarization algorithms and the proposed method 
%(From left to right Original, MRF, Kittler, Otsu, Niblack, Sauvola, Proposed)
%Text containing shadowed background
%}
%\label{fig:6}
%\end{figure*}
%\begin{figure*}[t]
%\centering
\subfigure{
\includegraphics[height=.6in,width=1.8in]{results/res_2/7/orig.eps}
\includegraphics[height=.6in,width=1.8in]{results/res_2/7/mrf.eps}
%\subfigure{
%\includegraphics[height=.5in,width=1in]{results/res_2/7/kit.eps}
%}
%\subfigure{
%\includegraphics[height=.5in,width=1in]{results/res_2/7/otsu.eps}
%}
%\subfigure{
%\includegraphics[height=.5in,width=1in]{results/res_2/7/nib.eps}
%}
%\includegraphics[height=.55in,width=1.3in]{results/res_2/7/graph.eps}
\includegraphics[height=.7in,width=1.8in]{results/res_2/7/res.eps}
}
%\\
%\label{fig:subfig11}
\subfigure{
\includegraphics[height=.6in,width=1.8in]{results/res_2/8/orig.eps}
\includegraphics[height=.6in,width=1.8in]{results/res_2/8/mrf.eps}
%\subfigure{
%\includegraphics[height=.4in,width=1in]{results/res_2/8/kit.eps}
%}
%\subfigure{
%\includegraphics[height=.4in,width=1in]{results/res_2/8/otsu.eps}
%}
%\subfigure{
%\includegraphics[height=.4in,width=1in]{results/res_2/8/nib.eps}
%}
%\includegraphics[height=.7in,width=1.8in]{results/res_2/8/graph.eps}
\includegraphics[height=.7in,width=1.8in]{results/res_2/8/res.eps}
}
%\\
%\label{fig:subfig11}
\subfigure{
\includegraphics[height=.6in,width=1.8in]{results/res_2/9/orig.eps}
\includegraphics[height=.6in,width=1.8in]{results/res_2/9/mrf.eps}
%\subfigure{
%\includegraphics[height=.4in,width=1in]{results/res_2/9/kit.eps}
%}
%\subfigure{
%\includegraphics[height=.4in,width=1in]{results/res_2/9/otsu.eps}
%}
%\subfigure{
%\includegraphics[height=.4in,width=1in]{results/res_2/9/nib.eps}
%}
%\includegraphics[height=.7in,width=1.8in]{results/res_2/9/graph.eps}
\includegraphics[height=.6in,width=1.8in]{results/res_2/9/res.eps}
}
%\\
%\label{fig:subfig11}
\subfigure{
\includegraphics[height=.6in,width=1.8in]{results/res_2/10/orig.eps}
\includegraphics[height=.6in,width=1.8in]{results/res_2/10/mrf.eps}
%\subfigure{
%\includegraphics[height=.5in,width=1in]{results/res_2/10/kit.eps}
%}
%\subfigure{
%\includegraphics[height=.5in,width=1in]{results/res_2/10/otsu.eps}
%}
%\subfigure{
%\includegraphics[height=.5in,width=1in]{results/res_2/10/nib.eps}
%}
%\includegraphics[height=.55in,width=1.3in]{results/res_2/10/graph.eps}
\includegraphics[height=.6in,width=1.8in]{results/res_2/10/res.eps}
}
%\\
%\label{fig:subfig11}
\subfigure{
\includegraphics[height=1in,width=1.8in]{results/res_2/11/orig.eps}
\includegraphics[height=1in,width=1.8in]{results/res_2/11/mrf.eps}
%\subfigure{
%\includegraphics[height=.7in,width=1in]{results/res_2/11/kit.eps}
%}
%\subfigure{
%\includegraphics[height=.7in,width=1in]{results/res_2/11/otsu.eps}
%\includegraphics[height=.7in,width=1in]{results/res_2/11/otsu.eps}
%}
%\subfigure{
%\includegraphics[height=.7in,width=1in]{results/res_2/11/nib.eps}
%}
%\subfigure{
%\includegraphics[height=.7in,width=1.3in]{results/res_2/11/graph.eps}
\includegraphics[height=1in,width=1.8in]{results/res_2/11/res.eps}
}
\label{fig:subfig11}
\caption
{Comparison of Binarization algorithms and the proposed method 
(From left to right Original, MRF, Proposed)}
\label{fig:proposed}
\end{figure*}
We took around 50
images from the dataset and generated its ground truth images for pixel level accuracy.
Some of these images are shown in Fig \ref{fig:ground}.
We use well known measures like precision, recall and F-score (Fig 4.9) %\ref{fig:accuracy}) 
to compare the proposed method with different binarization methods (Table I).
We also use OCR accuracy to show the effectiveness of our method. Note that we are only
using the subset of images that are most degraded by shadowing, illumination variations,
noise and specular reflections. The results of thresholding schemes are too poor for the
OCR algorithm to give any output. Therefore we only compare with the recent MRF~\cite{A16}
based model as shown in Table II. Some OCR results are shown in Fig 4.10. %\ref{fig:ocr}.

\begin{figure*}[t]
\centering
\subfigure[]{
\includegraphics[height=.7in,width=2.2in]{results/res_7/1.eps}
}
\subfigure[]{
\includegraphics[height=.7in,width=2.2in]{results/res_7/2.eps}
}
\subfigure[]{
\includegraphics[height=.7in,width=2.2in]{results/res_7/3.eps}
}
\subfigure[]{
\includegraphics[height=.7in,width=2.2in]{results/res_7/4.eps}
}
\label{fig:text}
\caption
{(a) Image containing Text over another Text (b) Foreground Text (c) Background Text (d)
Text extracted}
\subfigure[]{
\includegraphics[height=.6in,width=1.7in]{results/res_6/1.eps}
\includegraphics[height=.6in,width=1.7in]{results/res_6/2.eps}
}
\subfigure[]{
\includegraphics[height=.6in,width=1.7in]{results/res_6/3.eps}
\includegraphics[height=.6in,width=1.7in]{results/res_6/4.eps}
\label{fig:subfig11}
}
\caption
{Failure case where (a) Both the foreground and background are
of same color (b) Different Colored Text}
\label{fig:failure}
\end{figure*}

The results show that the proposed method is an effective method and performs better than other methods 
in the case where images have
complex background. The qualitative results are shown in Fig. \ref{fig:proposed}. Fig 4.12 shows that our 
technique can
also be applied to images containing text over another text. 
We analyze that the above methods do not work in the case where there is a complex and textured background in the images.
It is not that these methods do not work at all. No single algorithm works well for all types of images. Thus we can say
that our method can extract out the text embedded in complex reflective, shadowed and
specular background.
The failure case of our method is shown in Fig. \ref{fig:failure}.
Our method fails in cases where foreground text and the background are of the same color
and cases where text is composed of different colored letters.
Moreover, the approach works only with color images.

\section{Applications}

\subsection{Inscribed Text Segmentation}

Inscribed text is difficult to extract from one image as both the foreground and the background is of same 
color.
The inscriptions are generally found engraved/carved into stones, marble, metal or wood.
Extracted of the text is important as we want to preserve our historical writings which are currently being decayed.
However, due to effects of uncontrolled lighting condition and degradation of the material of the inscribed text,
extracting text from these images has become challenging problem. Fig \ref{fig:inscribe} shows the inscribed text images.
Hence we propose an efficient technique which can solve this problem.
\begin{figure}[t]
\centering
\subfigure{
\includegraphics[height=1.5in,width=1.8in]{chap4/res_4/try.eps}
}
\subfigure{
\includegraphics[height=1.5in,width=1.8in]{chap4/res_3/orig.eps}
}
\label{fig:subfig11}
\caption
{Inscribed Text image where both background and foreground are of same color}
\label{fig:inscribe}
%\end{figure}
%\begin{figure}[htbp]
%\vspace{10mm}
\centering
\subfigure[]{
\includegraphics[height=1.5in,width=1.8in]{chap4/res_2/orig.eps}
}
\subfigure[]{
\includegraphics[height=1.5in,width=1.8in]{chap4/res_2/direct.eps}
}
\subfigure[]{
\includegraphics[height=1.5in,width=1.8in]{chap4/res_2/global.eps}
}
\label{fig:subfig11}
\caption
{(a) Sponge Texture (b),(c) Independent components}
\label{fig:IC}
\end{figure}
\begin{figure}[h]
\centering
\subfigure[]{
\includegraphics[height=1.5in,width=1.8in]{chap4/res_1/orig.eps}
}
\subfigure[]{
\includegraphics[height=1.5in,width=1.8in]{chap4/res_1/ica_1.eps}
}
\subfigure[]{
\includegraphics[height=1.5in,width=1.8in]{chap4/res_1/ica_2.eps}
\label{fig:subfig11}
}
\caption
{(a) Image containing text (b),(c) Independent components}
\label{fig:ictext}
\end{figure}
\begin{figure}[p]
\centering
\subfigure[Original text ]{
\includegraphics[height=1.5in,width=1.6in]{chap4/res_3/orig.eps}
}
\subfigure[Kittler]{
\includegraphics[height=1.5in,width=1.6in]{chap4/res_3/kit.eps}
}
\subfigure[Otsu]{
\includegraphics[height=1.5in,width=1.6in]{chap4/res_3/otsu.eps}
}
\subfigure[Niblack]{
\includegraphics[height=1.5in,width=1.6in]{chap4/res_3/nib.eps}
}
\subfigure[Sauvola]{
\includegraphics[height=1.5in,width=1.6in]{chap4/res_3/sau.eps}
}
\subfigure[ICA + CBM]{
\includegraphics[height=1.5in,width=1.6in]{chap4/res_3/proposed.eps}
}
%\includegraphics[height=.5in,width=1.6in]{chap4/res_1/ica_2.eps}
\label{fig:res1}
\caption
{Binarized text}
%\end{figure}
%\begin{figure}[t]
\centering
\subfigure[Original text ]{
\includegraphics[height=1.5in,width=1.6in]{chap4/res_6/orig.eps}
}
\subfigure[Kittler]{
\includegraphics[height=1.5in,width=1.6in]{chap4/res_6/kit.eps}
}
\subfigure[Otsu]{
\includegraphics[height=1.5in,width=1.6in]{chap4/res_6/otsu.eps}
}
\subfigure[Niblack]{
\includegraphics[height=1.5in,width=1.6in]{chap4/res_6/nib.eps}
}
\subfigure[Sauvola]{
\includegraphics[height=1.5in,width=1.6in]{chap4/res_6/sau.eps}
}
\subfigure[ICA + CBM]{
\includegraphics[height=1.5in,width=1.6in]{chap4/res_6/proposed.eps}
}
%\includegraphics[height=.5in,width=1.6in]{chap4/res_1/ica_2.eps}
\label{fig:res2}
\caption
{Binarized text}
\end{figure}
First we capture multiple images of the inscribed text and apply our model to extract the text with the help of shadows.
For separating the global and the direct component, we used high frequency checkerboard pattern. 
This takes too much time as we have to capture many images.(Figure \ref{fig:IC} shows
the independent component of the sponge texture)
But for this, we apply an Independent Component Analysis (ICA) based method to the images captured containing text.
This method helps in extracting out the shadows (Figure \ref{fig:ictext}). 
Then we apply our component based model to efficiently binarize the text
embedded (Figure 4.17, 4.18).

\subsection{Enhancing Edge Detection}



Edge/Boundary detection is a fundamental task in computer vision with its applications 
in areas such as image segmentation, object recognition, object description, feature detection and extraction. 
It refers to the process of identifying points in an image where there are sharp discontinuities or
the image brightness changes sharply.
These points are ordered into a set of curved line segments known as edges.
The abrupt changes in pixel intensity values characterizes boundaries of objects in a scene.
The amount of data to be processed significantly reduces when we apply an edge detection algorithm.
This helps in filtering out data which is less important and still preserving the structural
properties of the image.
The task of interpreting the important information content in the original 
image may be simplified if the edge detection step is successful. However,
due to complexities in real scene images, it is not always possible to obtain 
clear edges which define the boundary of the object. Edge detection process is particularly sensitive to
noise and uneven illumination conditions. There are many edge detection algorithms available.
Each are sensitive to certain types of edges.
\begin{figure*}[t]
\centering
\subfigure{
\includegraphics[height=.7in,width=1.7in]{chap4/edge_detect/15.eps}
}
\subfigure{
\includegraphics[height=.7in,width=1.7in]{chap4/edge_detect/8.eps}
}
\subfigure{
\includegraphics[height=.7in,width=1.7in]{chap4/edge_detect/1.eps}
}
\subfigure{
\includegraphics[height=.7in,width=1.7in]{chap4/edge_detect/16.eps}
}
\subfigure{
\includegraphics[height=.7in,width=1.7in]{chap4/edge_detect/9.eps}
}
\subfigure{
\includegraphics[height=.7in,width=1.7in]{chap4/edge_detect/2.eps}
}
\subfigure{
\includegraphics[height=.7in,width=1.7in]{chap4/edge_detect/17.eps}
}
\subfigure{
\includegraphics[height=.7in,width=1.7in]{chap4/edge_detect/10.eps}
}
\subfigure{
\includegraphics[height=.7in,width=1.7in]{chap4/edge_detect/4.eps}
}
\subfigure{
\includegraphics[height=.7in,width=1.7in]{chap4/edge_detect/18.eps}
}
\subfigure{
\includegraphics[height=.7in,width=1.7in]{chap4/edge_detect/11.eps}
}
\subfigure{
\includegraphics[height=.7in,width=1.7in]{chap4/edge_detect/5.eps}
}
\subfigure{
\includegraphics[height=.7in,width=1.7in]{chap4/edge_detect/21.eps}
}
\subfigure{
\includegraphics[height=.7in,width=1.7in]{chap4/edge_detect/14.eps}
}
\subfigure{
\includegraphics[height=.7in,width=1.7in]{chap4/edge_detect/7.eps}
}
%\label{fig:subfig11}
\caption
{From left to right: Text image, Applying canny edge detector, Applying ICA model + canny edge detector}
\label{fig:edge}
\end{figure*}
Many models \cite{chap4-6, chap4-7} use edge detection as the first step 
to extract text from scene images. But the performance of this step decreases when 
complex background is present in the image.  
Here we will show how we use our ICA model to correctly identify the word boundaries.
First we use canny edge detector \cite{chap4-5} to detect edges from natural scene text images. 
When we directly apply canny edge detector on scene text images, we observe discontinuity in word boundaries.
But when we use our ICA model followed the canny edge detector, the edge boundaries becomes much more cleaner.
Fig \ref{fig:edge} shows the effectiveness of our approach.

\begin{figure*}[t]
\centering
\subfigure{
\includegraphics[height=.8in,width=1.3in]{chap4/shadow/1.eps}
}
\subfigure{
\includegraphics[height=.8in,width=1.3in]{chap4/shadow/2.eps}
}
\subfigure{
\includegraphics[height=.8in,width=1.3in]{chap4/shadow/3.eps}
}
\subfigure{
\includegraphics[height=.8in,width=1.3in]{chap4/shadow/4.eps}
}
\subfigure{
\includegraphics[height=.8in,width=1.3in]{chap4/shadow/5.eps}
}
\subfigure{
\includegraphics[height=.8in,width=1.3in]{chap4/shadow/6.eps}
}
\subfigure{
\includegraphics[height=.8in,width=1.3in]{chap4/shadow/7.eps}
}
\subfigure{
\includegraphics[height=.8in,width=1.3in]{chap4/shadow/8.eps}
}
\subfigure{
\includegraphics[height=.8in,width=1.3in]{chap4/shadow/9.eps}
}
\subfigure{
\includegraphics[height=.8in,width=1.3in]{chap4/shadow/10.eps}
}
\subfigure{
\includegraphics[height=.8in,width=1.3in]{chap4/shadow/11.eps}
}
\subfigure{
\includegraphics[height=.8in,width=1.3in]{chap4/shadow/12.eps}
}
%\label{fig:subfig11}
\caption
{Scene images containing shadows and their corresponding shadow masks}
\label{fig:shadow}
\end{figure*}

\subsection{Shadow Detection}
Shadows are generally created when objects obscure the light source. 
Detected shadows provide information about
lighting direction and scene geometry.
Due to the presence of shadows many computer vision 
tasks such as object detection, segmentation, scene analysis, tracking, etc are affected.
For example, the shadows may assign false segments in the 
image segmentation process. They may be wrongly detected as objects in 
object detection algorithms.
For these reasons,
shadow detection is an important component in scene interpretation.
Hence 
shadow detection and removal is an important preprocessing step for improving the performance of such vision tasks.
But decomposition of the image into a shadow image and a non-shadow image
is a difficult problem due to 
complex interactions of geometry and illumination.
Various pixel-based and region-based methods are
proposed to detect the shadows in an image \cite{chap4-8, chap4-9, chap4-10, chap4-11}.
Here we show that our ICA model can be used to detect shadows and create shadow masks from the scene images.
Fig \ref{fig:shadow} shows the shadow detected results. We first apply our ICA model followed by
global binarization method. Our method does not give good results if some dark objects are present in the scene
images which are mistaken as shadows.

\section{Summary}

In this chapter, we discussed the workflow of natural scene text segmentation.
We saw that that text segmentation 
from scene images is a challenging task due to the variations in color, size, and font
of the text. The results are often affected by complex backgrounds, 
different lighting conditions, shadows and reflections.
We used Independent Component Analysis (ICA) model to map out the text
region, which is inherently uniform in nature, while removing
shadows, specularity and reflections, which were included in
the background.
We first decomposed the images into sub-components i.e Red, Green and Blue channels which
were taken as the observations of ICA model.
This enabled us to separate text from complex backgrounds. 
Then to get a clean binary image, we applied a global thresholding method on the independent components of the image
and that with maximum textual properties was used for extracting the foreground text. Binarization results
on ICDAR dataset showed 
significant improvement in the extraction of text over other previously reported methods.
We also showed how our method can be used for different applications like inscribed text
segmentation, enhancing edge detection and for shadow detection in scene images.



